{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "PXuTJXRrCiAP",
    "colab_type": "code",
    "outputId": "941414de-e08b-48fc-9d89-33f4d92125ee",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.563168085053E12,
     "user_tz": 240.0,
     "elapsed": 93192.0,
     "user": {
      "displayName": "胡沐晗",
      "photoUrl": "",
      "userId": "13480278940063465236"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Package 'python-software-properties' has no installation candidate\n",
      "Selecting previously unselected package google-drive-ocamlfuse.\n",
      "(Reading database ... 130963 files and directories currently installed.)\n",
      "Preparing to unpack .../google-drive-ocamlfuse_0.7.6-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
      "Unpacking google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n",
      "Setting up google-drive-ocamlfuse (0.7.6-0ubuntu1~ubuntu18.04.1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "··········\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "Please enter the verification code: Access token retrieved correctly.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tWmQNHwZDPJ5",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "id": "6EQt0njYCrKX",
    "outputId": "23b05d80-ec3b-4f8e-a6bd-b8da5368c1ba",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.56320112247E12,
     "user_tz": 240.0,
     "elapsed": 3168455.0,
     "user": {
      "displayName": "胡沐晗",
      "photoUrl": "",
      "userId": "13480278940063465236"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:201: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:202: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1193515, 50)\n",
      "Reading dataset\n",
      "Total stances: 49972\n",
      "Total bodies: 1683\n",
      "Reading dataset\n",
      "Total stances: 25413\n",
      "Total bodies: 904\n",
      "X_train_head.shape = (25413, 16)\n",
      "X_train_doc.shape = (25413, 28)\n",
      "X_train_head.shape = (9622, 16)\n",
      "X_train_doc.shape = (9622, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0715 13:40:50.391449 140357147805568 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "W0715 13:40:50.413933 140357147805568 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0715 13:40:50.415360 140357147805568 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0715 13:40:50.432371 140357147805568 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0715 13:40:50.433699 140357147805568 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_head.shape = (36226, 16)\n",
      "X_train_doc.shape = (36226, 28)\n",
      "X_train_head.shape = (4124, 16)\n",
      "X_train_doc.shape = (4124, 28)\n",
      "{0: 3.5585461689587428, 1: 15.298141891891891, 2: 1.3680513595166164, 3: 0.342154973742869}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 13:40:51.701420 140357147805568 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0715 13:40:51.930353 140357147805568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0715 13:40:52.407054 140357147805568 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(36226, 44)\n",
      "ytrain shape:(36226,)\n",
      "Train on 36226 samples, validate on 4124 samples\n",
      "Epoch 1/100\n",
      "36226/36226 [==============================] - 14s 391us/step - loss: 0.5247 - acc: 0.8236 - val_loss: 0.4379 - val_acc: 0.8363\n",
      " - fnc_score: 0.721057167480609\n",
      " - val_f1_(macro): 0.38685988575518593\n",
      "\n",
      "\n",
      "Epoch 2/100\n",
      "  500/36226 [..............................] - ETA: 11s - loss: 0.4058 - acc: 0.8480"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36226/36226 [==============================] - 12s 322us/step - loss: 0.4203 - acc: 0.8545 - val_loss: 0.4382 - val_acc: 0.8356\n",
      " - fnc_score: 0.7471990807239299\n",
      " - val_f1_(macro): 0.4161939331804124\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "36226/36226 [==============================] - 12s 318us/step - loss: 0.3971 - acc: 0.8624 - val_loss: 0.3817 - val_acc: 0.8538\n",
      " - fnc_score: 0.7365699511634588\n",
      " - val_f1_(macro): 0.41110010089304766\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "36226/36226 [==============================] - 12s 321us/step - loss: 0.3806 - acc: 0.8676 - val_loss: 0.3716 - val_acc: 0.8548\n",
      " - fnc_score: 0.753231829933927\n",
      " - val_f1_(macro): 0.4332590481126371\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "36226/36226 [==============================] - 12s 323us/step - loss: 0.3725 - acc: 0.8705 - val_loss: 0.3474 - val_acc: 0.8615\n",
      " - fnc_score: 0.7565354783108302\n",
      " - val_f1_(macro): 0.4447170062136664\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "36226/36226 [==============================] - 12s 322us/step - loss: 0.3624 - acc: 0.8727 - val_loss: 0.3648 - val_acc: 0.8608\n",
      " - fnc_score: 0.7661591496696352\n",
      " - val_f1_(macro): 0.4328945749946794\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "36226/36226 [==============================] - 12s 323us/step - loss: 0.3597 - acc: 0.8748 - val_loss: 0.3559 - val_acc: 0.8620\n",
      " - fnc_score: 0.7671646078713014\n",
      " - val_f1_(macro): 0.44324659842972824\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "36226/36226 [==============================] - 12s 322us/step - loss: 0.3526 - acc: 0.8764 - val_loss: 0.3454 - val_acc: 0.8640\n",
      " - fnc_score: 0.7677391554151106\n",
      " - val_f1_(macro): 0.4544786891096254\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "36226/36226 [==============================] - 12s 323us/step - loss: 0.3491 - acc: 0.8781 - val_loss: 0.3569 - val_acc: 0.8649\n",
      " - fnc_score: 0.7525136455041654\n",
      " - val_f1_(macro): 0.42908409764886996\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "36226/36226 [==============================] - 12s 319us/step - loss: 0.3435 - acc: 0.8787 - val_loss: 0.3351 - val_acc: 0.8661\n",
      " - fnc_score: 0.7618500430910657\n",
      " - val_f1_(macro): 0.4464698762107055\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "36226/36226 [==============================] - 12s 322us/step - loss: 0.3394 - acc: 0.8793 - val_loss: 0.3404 - val_acc: 0.8657\n",
      " - fnc_score: 0.7742028152829646\n",
      " - val_f1_(macro): 0.4555643961040402\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "36226/36226 [==============================] - 12s 319us/step - loss: 0.3400 - acc: 0.8796 - val_loss: 0.3444 - val_acc: 0.8652\n",
      " - fnc_score: 0.7663027865555875\n",
      " - val_f1_(macro): 0.43954759931682624\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "36226/36226 [==============================] - 12s 321us/step - loss: 0.3329 - acc: 0.8821 - val_loss: 0.3282 - val_acc: 0.8703\n",
      " - fnc_score: 0.7665900603274921\n",
      " - val_f1_(macro): 0.45727235205559646\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "36226/36226 [==============================] - 12s 322us/step - loss: 0.3282 - acc: 0.8830 - val_loss: 0.3367 - val_acc: 0.8674\n",
      " - fnc_score: 0.7654409652398736\n",
      " - val_f1_(macro): 0.4452226468590953\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "36226/36226 [==============================] - 12s 323us/step - loss: 0.3266 - acc: 0.8832 - val_loss: 0.3290 - val_acc: 0.8671\n",
      " - fnc_score: 0.7579718471703534\n",
      " - val_f1_(macro): 0.4421916769465335\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (4124, 16)\n",
      "X_train_doc.shape = (4124, 28)\n",
      "Score for fold 6 was - 0.7579718471703534\n",
      "X_train_head.shape = (35687, 16)\n",
      "X_train_doc.shape = (35687, 28)\n",
      "X_train_head.shape = (4663, 16)\n",
      "X_train_doc.shape = (4663, 28)\n",
      "{0: 3.436729583975347, 1: 14.2748, 2: 1.4123397182206743, 3: 0.3411889556005966}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(35687, 44)\n",
      "ytrain shape:(35687,)\n",
      "Train on 35687 samples, validate on 4663 samples\n",
      "Epoch 1/100\n",
      "35687/35687 [==============================] - 14s 394us/step - loss: 0.5264 - acc: 0.8246 - val_loss: 0.4609 - val_acc: 0.8392\n",
      " - fnc_score: 0.7490806570237803\n",
      " - val_f1_(macro): 0.40890741025814364\n",
      "\n",
      "\n",
      "Epoch 2/100\n",
      "  500/35687 [..............................] - ETA: 10s - loss: 0.4434 - acc: 0.8300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35687/35687 [==============================] - 12s 322us/step - loss: 0.4208 - acc: 0.8520 - val_loss: 0.4105 - val_acc: 0.8570\n",
      " - fnc_score: 0.7668546212306938\n",
      " - val_f1_(macro): 0.42597838587493564\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "35687/35687 [==============================] - 11s 321us/step - loss: 0.3957 - acc: 0.8611 - val_loss: 0.4020 - val_acc: 0.8550\n",
      " - fnc_score: 0.7849963226280952\n",
      " - val_f1_(macro): 0.45404154992129214\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "35687/35687 [==============================] - 11s 322us/step - loss: 0.3768 - acc: 0.8659 - val_loss: 0.3887 - val_acc: 0.8694\n",
      " - fnc_score: 0.7759254719293944\n",
      " - val_f1_(macro): 0.44639361843186465\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "35687/35687 [==============================] - 11s 321us/step - loss: 0.3721 - acc: 0.8684 - val_loss: 0.3805 - val_acc: 0.8610\n",
      " - fnc_score: 0.7902672223584212\n",
      " - val_f1_(macro): 0.44457781858436496\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "35687/35687 [==============================] - 12s 325us/step - loss: 0.3644 - acc: 0.8707 - val_loss: 0.3686 - val_acc: 0.8726\n",
      " - fnc_score: 0.7929639617553322\n",
      " - val_f1_(macro): 0.473685329641122\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "35687/35687 [==============================] - 11s 320us/step - loss: 0.3591 - acc: 0.8731 - val_loss: 0.3588 - val_acc: 0.8767\n",
      " - fnc_score: 0.7927188036283402\n",
      " - val_f1_(macro): 0.4767302986546972\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "35687/35687 [==============================] - 12s 324us/step - loss: 0.3523 - acc: 0.8749 - val_loss: 0.3468 - val_acc: 0.8778\n",
      " - fnc_score: 0.7822995832311841\n",
      " - val_f1_(macro): 0.4717598564820835\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "35687/35687 [==============================] - 11s 320us/step - loss: 0.3490 - acc: 0.8750 - val_loss: 0.3556 - val_acc: 0.8767\n",
      " - fnc_score: 0.7759254719293944\n",
      " - val_f1_(macro): 0.4629285608593332\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "35687/35687 [==============================] - 11s 321us/step - loss: 0.3433 - acc: 0.8773 - val_loss: 0.3613 - val_acc: 0.8773\n",
      " - fnc_score: 0.7722481000245158\n",
      " - val_f1_(macro): 0.46566189803619584\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "35687/35687 [==============================] - 12s 323us/step - loss: 0.3435 - acc: 0.8764 - val_loss: 0.3448 - val_acc: 0.8782\n",
      " - fnc_score: 0.7932091198823241\n",
      " - val_f1_(macro): 0.47466157194188163\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "35687/35687 [==============================] - 11s 320us/step - loss: 0.3358 - acc: 0.8794 - val_loss: 0.3455 - val_acc: 0.8745\n",
      " - fnc_score: 0.7961510174062271\n",
      " - val_f1_(macro): 0.4848266328620833\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "35687/35687 [==============================] - 11s 319us/step - loss: 0.3344 - acc: 0.8794 - val_loss: 0.3402 - val_acc: 0.8793\n",
      " - fnc_score: 0.7905123804854131\n",
      " - val_f1_(macro): 0.4795525168112789\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "35687/35687 [==============================] - 11s 321us/step - loss: 0.3311 - acc: 0.8794 - val_loss: 0.3520 - val_acc: 0.8784\n",
      " - fnc_score: 0.7911252758028928\n",
      " - val_f1_(macro): 0.4816307899543544\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "35687/35687 [==============================] - 11s 321us/step - loss: 0.3262 - acc: 0.8812 - val_loss: 0.3459 - val_acc: 0.8778\n",
      " - fnc_score: 0.7884285364059819\n",
      " - val_f1_(macro): 0.483324813938817\n",
      "\n",
      "\n",
      "Epoch 16/100\n",
      "35687/35687 [==============================] - 11s 321us/step - loss: 0.3231 - acc: 0.8818 - val_loss: 0.3520 - val_acc: 0.8790\n",
      " - fnc_score: 0.7754351556754107\n",
      " - val_f1_(macro): 0.4690586179171168\n",
      "\n",
      "\n",
      "Epoch 17/100\n",
      "35687/35687 [==============================] - 11s 321us/step - loss: 0.3221 - acc: 0.8819 - val_loss: 0.3464 - val_acc: 0.8803\n",
      " - fnc_score: 0.7911252758028928\n",
      " - val_f1_(macro): 0.49299045231802774\n",
      "\n",
      "\n",
      "Epoch 18/100\n",
      "35687/35687 [==============================] - 11s 321us/step - loss: 0.3189 - acc: 0.8838 - val_loss: 0.3537 - val_acc: 0.8805\n",
      " - fnc_score: 0.7919833292473646\n",
      " - val_f1_(macro): 0.4877357106088209\n",
      "\n",
      "\n",
      "Epoch 19/100\n",
      "35687/35687 [==============================] - 12s 323us/step - loss: 0.3154 - acc: 0.8834 - val_loss: 0.3485 - val_acc: 0.8808\n",
      " - fnc_score: 0.7990929149301299\n",
      " - val_f1_(macro): 0.5015365878626746\n",
      "\n",
      "\n",
      "Epoch 20/100\n",
      "35687/35687 [==============================] - 12s 325us/step - loss: 0.3110 - acc: 0.8853 - val_loss: 0.3430 - val_acc: 0.8775\n",
      " - fnc_score: 0.7968864917872027\n",
      " - val_f1_(macro): 0.4854801731788611\n",
      "\n",
      "\n",
      "Epoch 21/100\n",
      "35687/35687 [==============================] - 11s 320us/step - loss: 0.3079 - acc: 0.8865 - val_loss: 0.3482 - val_acc: 0.8793\n",
      " - fnc_score: 0.7928413826918362\n",
      " - val_f1_(macro): 0.48393964685529717\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (4663, 16)\n",
      "X_train_doc.shape = (4663, 28)\n",
      "Score for fold 0 was - 0.7928413826918362\n",
      "X_train_head.shape = (36567, 16)\n",
      "X_train_doc.shape = (36567, 28)\n",
      "X_train_head.shape = (3783, 16)\n",
      "X_train_doc.shape = (3783, 28)\n",
      "{0: 3.4136482449589245, 1: 15.03577302631579, 2: 1.44625059326056, 3: 0.3390856824925816}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(36567, 44)\n",
      "ytrain shape:(36567,)\n",
      "Train on 36567 samples, validate on 3783 samples\n",
      "Epoch 1/100\n",
      "36567/36567 [==============================] - 15s 403us/step - loss: 0.5219 - acc: 0.8278 - val_loss: 0.4615 - val_acc: 0.8401\n",
      " - fnc_score: 0.7359637957856031\n",
      " - val_f1_(macro): 0.4389966722082572\n",
      "\n",
      "\n",
      "Epoch 2/100\n",
      "36567/36567 [==============================] - 12s 329us/step - loss: 0.4202 - acc: 0.8521 - val_loss: 0.4319 - val_acc: 0.8536\n",
      " - fnc_score: 0.7448734266723236\n",
      " - val_f1_(macro): 0.4836458906243225\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "  500/36567 [..............................] - ETA: 10s - loss: 0.3844 - acc: 0.8700"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36567/36567 [==============================] - 12s 326us/step - loss: 0.3904 - acc: 0.8624 - val_loss: 0.4184 - val_acc: 0.8549\n",
      " - fnc_score: 0.7512374487342667\n",
      " - val_f1_(macro): 0.47011391578614503\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "36567/36567 [==============================] - 12s 325us/step - loss: 0.3776 - acc: 0.8664 - val_loss: 0.4361 - val_acc: 0.8610\n",
      " - fnc_score: 0.796068448592844\n",
      " - val_f1_(macro): 0.49819190331837776\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "36567/36567 [==============================] - 12s 326us/step - loss: 0.3694 - acc: 0.8691 - val_loss: 0.4140 - val_acc: 0.8599\n",
      " - fnc_score: 0.7923914580681658\n",
      " - val_f1_(macro): 0.49568094950035596\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "36567/36567 [==============================] - 12s 327us/step - loss: 0.3604 - acc: 0.8712 - val_loss: 0.4243 - val_acc: 0.8567\n",
      " - fnc_score: 0.7969169848677697\n",
      " - val_f1_(macro): 0.4983702594137326\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "36567/36567 [==============================] - 12s 326us/step - loss: 0.3548 - acc: 0.8730 - val_loss: 0.4103 - val_acc: 0.8631\n",
      " - fnc_score: 0.7703295149200962\n",
      " - val_f1_(macro): 0.4819561526938782\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "36567/36567 [==============================] - 12s 327us/step - loss: 0.3494 - acc: 0.8747 - val_loss: 0.4004 - val_acc: 0.8684\n",
      " - fnc_score: 0.7878659312685617\n",
      " - val_f1_(macro): 0.49167604354307737\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "36567/36567 [==============================] - 12s 326us/step - loss: 0.3437 - acc: 0.8764 - val_loss: 0.3997 - val_acc: 0.8670\n",
      " - fnc_score: 0.7936642624805543\n",
      " - val_f1_(macro): 0.49433600710161374\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "36567/36567 [==============================] - 12s 327us/step - loss: 0.3412 - acc: 0.8770 - val_loss: 0.4102 - val_acc: 0.8620\n",
      " - fnc_score: 0.792674303493141\n",
      " - val_f1_(macro): 0.4961664345561613\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "36567/36567 [==============================] - 12s 327us/step - loss: 0.3342 - acc: 0.8792 - val_loss: 0.4186 - val_acc: 0.8623\n",
      " - fnc_score: 0.8131805968038467\n",
      " - val_f1_(macro): 0.4928829502001319\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "36567/36567 [==============================] - 12s 327us/step - loss: 0.3306 - acc: 0.8808 - val_loss: 0.4098 - val_acc: 0.8649\n",
      " - fnc_score: 0.8021496252298119\n",
      " - val_f1_(macro): 0.497811345750988\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "36567/36567 [==============================] - 12s 325us/step - loss: 0.3291 - acc: 0.8808 - val_loss: 0.4017 - val_acc: 0.8657\n",
      " - fnc_score: 0.7827747136190072\n",
      " - val_f1_(macro): 0.4947702249216519\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "36567/36567 [==============================] - 12s 328us/step - loss: 0.3249 - acc: 0.8807 - val_loss: 0.4037 - val_acc: 0.8665\n",
      " - fnc_score: 0.7895630038184133\n",
      " - val_f1_(macro): 0.49854328970617895\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "36567/36567 [==============================] - 12s 327us/step - loss: 0.3220 - acc: 0.8815 - val_loss: 0.3996 - val_acc: 0.8673\n",
      " - fnc_score: 0.7921086126431905\n",
      " - val_f1_(macro): 0.4961816891309697\n",
      "\n",
      "\n",
      "Epoch 16/100\n",
      "36567/36567 [==============================] - 12s 323us/step - loss: 0.3177 - acc: 0.8828 - val_loss: 0.4054 - val_acc: 0.8655\n",
      " - fnc_score: 0.7939471079055296\n",
      " - val_f1_(macro): 0.5015132160561524\n",
      "\n",
      "\n",
      "Epoch 17/100\n",
      "36567/36567 [==============================] - 12s 327us/step - loss: 0.3168 - acc: 0.8830 - val_loss: 0.4115 - val_acc: 0.8641\n",
      " - fnc_score: 0.7888558902559751\n",
      " - val_f1_(macro): 0.49829420563061555\n",
      "\n",
      "\n",
      "Epoch 18/100\n",
      "36567/36567 [==============================] - 12s 327us/step - loss: 0.3128 - acc: 0.8846 - val_loss: 0.4120 - val_acc: 0.8615\n",
      " - fnc_score: 0.7875830858435865\n",
      " - val_f1_(macro): 0.49313559256941175\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (3783, 16)\n",
      "X_train_doc.shape = (3783, 28)\n",
      "Score for fold 7 was - 0.7875830858435865\n",
      "X_train_head.shape = (36962, 16)\n",
      "X_train_doc.shape = (36962, 28)\n",
      "X_train_head.shape = (3388, 16)\n",
      "X_train_doc.shape = (3388, 28)\n",
      "{0: 3.5622590593677717, 1: 15.000811688311689, 2: 1.453137285736751, 3: 0.33733070492461575}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(36962, 44)\n",
      "ytrain shape:(36962,)\n",
      "Train on 36962 samples, validate on 3388 samples\n",
      "Epoch 1/100\n",
      "36962/36962 [==============================] - 15s 411us/step - loss: 0.5065 - acc: 0.8358 - val_loss: 0.5156 - val_acc: 0.8208\n",
      " - fnc_score: 0.7285714285714285\n",
      " - val_f1_(macro): 0.44032832359493146\n",
      "\n",
      "\n",
      "Epoch 2/100\n",
      "  500/36962 [..............................] - ETA: 11s - loss: 0.4252 - acc: 0.8580"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36962/36962 [==============================] - 12s 319us/step - loss: 0.4107 - acc: 0.8567 - val_loss: 0.5025 - val_acc: 0.8262\n",
      " - fnc_score: 0.7586156111929307\n",
      " - val_f1_(macro): 0.45792207049614997\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "36962/36962 [==============================] - 12s 317us/step - loss: 0.3901 - acc: 0.8630 - val_loss: 0.4669 - val_acc: 0.8338\n",
      " - fnc_score: 0.7371134020618557\n",
      " - val_f1_(macro): 0.4633429276101253\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "36962/36962 [==============================] - 12s 318us/step - loss: 0.3719 - acc: 0.8694 - val_loss: 0.4550 - val_acc: 0.8385\n",
      " - fnc_score: 0.7602356406480117\n",
      " - val_f1_(macro): 0.46992246642237034\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "36962/36962 [==============================] - 12s 318us/step - loss: 0.3638 - acc: 0.8712 - val_loss: 0.4858 - val_acc: 0.8297\n",
      " - fnc_score: 0.722680412371134\n",
      " - val_f1_(macro): 0.4595029159517664\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "36962/36962 [==============================] - 12s 319us/step - loss: 0.3583 - acc: 0.8741 - val_loss: 0.4546 - val_acc: 0.8424\n",
      " - fnc_score: 0.7673048600883653\n",
      " - val_f1_(macro): 0.4735248151108722\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "36962/36962 [==============================] - 12s 317us/step - loss: 0.3477 - acc: 0.8771 - val_loss: 0.4387 - val_acc: 0.8439\n",
      " - fnc_score: 0.7655375552282768\n",
      " - val_f1_(macro): 0.47671150996710004\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "36962/36962 [==============================] - 12s 317us/step - loss: 0.3442 - acc: 0.8768 - val_loss: 0.4385 - val_acc: 0.8436\n",
      " - fnc_score: 0.7683357879234168\n",
      " - val_f1_(macro): 0.4846017327525427\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "36962/36962 [==============================] - 12s 320us/step - loss: 0.3398 - acc: 0.8790 - val_loss: 0.4405 - val_acc: 0.8418\n",
      " - fnc_score: 0.7606774668630338\n",
      " - val_f1_(macro): 0.47852653343286705\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "36962/36962 [==============================] - 12s 316us/step - loss: 0.3370 - acc: 0.8801 - val_loss: 0.4832 - val_acc: 0.8362\n",
      " - fnc_score: 0.7424153166421208\n",
      " - val_f1_(macro): 0.482048626587923\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "36962/36962 [==============================] - 12s 319us/step - loss: 0.3348 - acc: 0.8808 - val_loss: 0.4414 - val_acc: 0.8430\n",
      " - fnc_score: 0.7512518409425626\n",
      " - val_f1_(macro): 0.48085711693414473\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "36962/36962 [==============================] - 12s 316us/step - loss: 0.3295 - acc: 0.8814 - val_loss: 0.4328 - val_acc: 0.8471\n",
      " - fnc_score: 0.7883652430044182\n",
      " - val_f1_(macro): 0.49245258644412204\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "36962/36962 [==============================] - 12s 319us/step - loss: 0.3280 - acc: 0.8813 - val_loss: 0.4329 - val_acc: 0.8436\n",
      " - fnc_score: 0.7759941089837997\n",
      " - val_f1_(macro): 0.4797087673629188\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "36962/36962 [==============================] - 12s 320us/step - loss: 0.3246 - acc: 0.8828 - val_loss: 0.4279 - val_acc: 0.8462\n",
      " - fnc_score: 0.76759941089838\n",
      " - val_f1_(macro): 0.4951158133762078\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "36962/36962 [==============================] - 12s 321us/step - loss: 0.3200 - acc: 0.8834 - val_loss: 0.4315 - val_acc: 0.8492\n",
      " - fnc_score: 0.777319587628866\n",
      " - val_f1_(macro): 0.483292503943463\n",
      "\n",
      "\n",
      "Epoch 16/100\n",
      "36962/36962 [==============================] - 12s 317us/step - loss: 0.3183 - acc: 0.8831 - val_loss: 0.4211 - val_acc: 0.8442\n",
      " - fnc_score: 0.7627393225331369\n",
      " - val_f1_(macro): 0.487129840152593\n",
      "\n",
      "\n",
      "Epoch 17/100\n",
      "36962/36962 [==============================] - 12s 320us/step - loss: 0.3132 - acc: 0.8856 - val_loss: 0.4234 - val_acc: 0.8442\n",
      " - fnc_score: 0.761561119293078\n",
      " - val_f1_(macro): 0.4888008160144649\n",
      "\n",
      "\n",
      "Epoch 18/100\n",
      "36962/36962 [==============================] - 12s 319us/step - loss: 0.3142 - acc: 0.8861 - val_loss: 0.4358 - val_acc: 0.8445\n",
      " - fnc_score: 0.7842415316642121\n",
      " - val_f1_(macro): 0.4798750758765771\n",
      "\n",
      "\n",
      "Epoch 19/100\n",
      "36962/36962 [==============================] - 12s 318us/step - loss: 0.3097 - acc: 0.8868 - val_loss: 0.4279 - val_acc: 0.8459\n",
      " - fnc_score: 0.7622974963181148\n",
      " - val_f1_(macro): 0.4957178224051427\n",
      "\n",
      "\n",
      "Epoch 20/100\n",
      "36962/36962 [==============================] - 12s 318us/step - loss: 0.3070 - acc: 0.8873 - val_loss: 0.4345 - val_acc: 0.8465\n",
      " - fnc_score: 0.7640648011782032\n",
      " - val_f1_(macro): 0.49446157976406957\n",
      "\n",
      "\n",
      "Epoch 21/100\n",
      "36962/36962 [==============================] - 12s 319us/step - loss: 0.3019 - acc: 0.8879 - val_loss: 0.4291 - val_acc: 0.8506\n",
      " - fnc_score: 0.7832106038291605\n",
      " - val_f1_(macro): 0.49194582980146984\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (3388, 16)\n",
      "X_train_doc.shape = (3388, 28)\n",
      "Score for fold 5 was - 0.7832106038291605\n",
      "X_train_head.shape = (36706, 16)\n",
      "X_train_doc.shape = (36706, 28)\n",
      "X_train_head.shape = (3644, 16)\n",
      "X_train_doc.shape = (3644, 28)\n",
      "{0: 3.4164184661206254, 1: 14.6824, 2: 1.4533576179917644, 3: 0.3388538089435397}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(36706, 44)\n",
      "ytrain shape:(36706,)\n",
      "Train on 36706 samples, validate on 3644 samples\n",
      "Epoch 1/100\n",
      "36706/36706 [==============================] - 16s 427us/step - loss: 0.5213 - acc: 0.8253 - val_loss: 0.4411 - val_acc: 0.8485\n",
      " - fnc_score: 0.7593777260831637\n",
      " - val_f1_(macro): 0.4206771574265509\n",
      "\n",
      "\n",
      "Epoch 2/100\n",
      "  500/36706 [..............................] - ETA: 11s - loss: 0.4875 - acc: 0.8300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36706/36706 [==============================] - 12s 323us/step - loss: 0.4171 - acc: 0.8525 - val_loss: 0.4120 - val_acc: 0.8650\n",
      " - fnc_score: 0.7896190753125909\n",
      " - val_f1_(macro): 0.48574704736097907\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "36706/36706 [==============================] - 12s 322us/step - loss: 0.3941 - acc: 0.8597 - val_loss: 0.3984 - val_acc: 0.8672\n",
      " - fnc_score: 0.7763884850247165\n",
      " - val_f1_(macro): 0.47194402323288354\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "36706/36706 [==============================] - 12s 319us/step - loss: 0.3783 - acc: 0.8653 - val_loss: 0.3819 - val_acc: 0.8729\n",
      " - fnc_score: 0.7880197731898808\n",
      " - val_f1_(macro): 0.46205229634857226\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "36706/36706 [==============================] - 12s 323us/step - loss: 0.3702 - acc: 0.8686 - val_loss: 0.3829 - val_acc: 0.8735\n",
      " - fnc_score: 0.7976155859261413\n",
      " - val_f1_(macro): 0.49990408289018573\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "36706/36706 [==============================] - 12s 322us/step - loss: 0.3606 - acc: 0.8714 - val_loss: 0.3725 - val_acc: 0.8768\n",
      " - fnc_score: 0.8070660075603373\n",
      " - val_f1_(macro): 0.4990662082548938\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "36706/36706 [==============================] - 12s 322us/step - loss: 0.3562 - acc: 0.8732 - val_loss: 0.3672 - val_acc: 0.8782\n",
      " - fnc_score: 0.8076475719685955\n",
      " - val_f1_(macro): 0.48632930175446376\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "36706/36706 [==============================] - 12s 322us/step - loss: 0.3505 - acc: 0.8739 - val_loss: 0.3640 - val_acc: 0.8762\n",
      " - fnc_score: 0.7929630706600757\n",
      " - val_f1_(macro): 0.48865829827643736\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "36706/36706 [==============================] - 12s 323us/step - loss: 0.3457 - acc: 0.8759 - val_loss: 0.3818 - val_acc: 0.8795\n",
      " - fnc_score: 0.8205873800523408\n",
      " - val_f1_(macro): 0.4880221837973552\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "36706/36706 [==============================] - 12s 320us/step - loss: 0.3450 - acc: 0.8765 - val_loss: 0.3803 - val_acc: 0.8809\n",
      " - fnc_score: 0.8099738296016283\n",
      " - val_f1_(macro): 0.4921212627547676\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "36706/36706 [==============================] - 12s 323us/step - loss: 0.3393 - acc: 0.8775 - val_loss: 0.3698 - val_acc: 0.8814\n",
      " - fnc_score: 0.8191334690316953\n",
      " - val_f1_(macro): 0.4931443545049279\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "36706/36706 [==============================] - 12s 324us/step - loss: 0.3331 - acc: 0.8795 - val_loss: 0.3652 - val_acc: 0.8834\n",
      " - fnc_score: 0.8214597266647281\n",
      " - val_f1_(macro): 0.49576465193033403\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "36706/36706 [==============================] - 12s 325us/step - loss: 0.3316 - acc: 0.8799 - val_loss: 0.3605 - val_acc: 0.8820\n",
      " - fnc_score: 0.8150625181738878\n",
      " - val_f1_(macro): 0.4745958362129532\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "36706/36706 [==============================] - 12s 325us/step - loss: 0.3288 - acc: 0.8811 - val_loss: 0.3752 - val_acc: 0.8771\n",
      " - fnc_score: 0.7957255015993021\n",
      " - val_f1_(macro): 0.4947428854263607\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "36706/36706 [==============================] - 12s 324us/step - loss: 0.3243 - acc: 0.8813 - val_loss: 0.3675 - val_acc: 0.8825\n",
      " - fnc_score: 0.8166618202965978\n",
      " - val_f1_(macro): 0.5062910498332269\n",
      "\n",
      "\n",
      "Epoch 16/100\n",
      "36706/36706 [==============================] - 12s 319us/step - loss: 0.3209 - acc: 0.8829 - val_loss: 0.3725 - val_acc: 0.8798\n",
      " - fnc_score: 0.8102646118057575\n",
      " - val_f1_(macro): 0.5086085839940304\n",
      "\n",
      "\n",
      "Epoch 17/100\n",
      "36706/36706 [==============================] - 12s 323us/step - loss: 0.3176 - acc: 0.8825 - val_loss: 0.3675 - val_acc: 0.8812\n",
      " - fnc_score: 0.8093922651933702\n",
      " - val_f1_(macro): 0.5075698392852335\n",
      "\n",
      "\n",
      "Epoch 18/100\n",
      "36706/36706 [==============================] - 12s 323us/step - loss: 0.3157 - acc: 0.8843 - val_loss: 0.3766 - val_acc: 0.8787\n",
      " - fnc_score: 0.8016865367839489\n",
      " - val_f1_(macro): 0.5051874759104698\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (3644, 16)\n",
      "X_train_doc.shape = (3644, 28)\n",
      "Score for fold 2 was - 0.8016865367839489\n",
      "X_train_head.shape = (35706, 16)\n",
      "X_train_doc.shape = (35706, 28)\n",
      "X_train_head.shape = (4644, 16)\n",
      "X_train_doc.shape = (4644, 28)\n",
      "{0: 3.3596161083929243, 1: 15.104060913705585, 2: 1.3995766698024459, 3: 0.342273773006135}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(35706, 44)\n",
      "ytrain shape:(35706,)\n",
      "Train on 35706 samples, validate on 4644 samples\n",
      "Epoch 1/100\n",
      "35706/35706 [==============================] - 16s 443us/step - loss: 0.5301 - acc: 0.8212 - val_loss: 0.3888 - val_acc: 0.8665\n",
      " - fnc_score: 0.782984126984127\n",
      " - val_f1_(macro): 0.42886116090760396\n",
      "\n",
      "\n",
      "Epoch 2/100\n",
      "35706/35706 [==============================] - 12s 325us/step - loss: 0.4243 - acc: 0.8522 - val_loss: 0.3796 - val_acc: 0.8831\n",
      " - fnc_score: 0.7954285714285714\n",
      " - val_f1_(macro): 0.47230086400498483\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "  500/35706 [..............................] - ETA: 10s - loss: 0.3852 - acc: 0.8740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35706/35706 [==============================] - 12s 324us/step - loss: 0.4022 - acc: 0.8576 - val_loss: 0.3549 - val_acc: 0.8848\n",
      " - fnc_score: 0.8105396825396826\n",
      " - val_f1_(macro): 0.48243985363450337\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "35706/35706 [==============================] - 12s 326us/step - loss: 0.3837 - acc: 0.8643 - val_loss: 0.3328 - val_acc: 0.8861\n",
      " - fnc_score: 0.8124444444444444\n",
      " - val_f1_(macro): 0.4468882181600655\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "35706/35706 [==============================] - 12s 327us/step - loss: 0.3754 - acc: 0.8675 - val_loss: 0.3328 - val_acc: 0.8882\n",
      " - fnc_score: 0.8191746031746032\n",
      " - val_f1_(macro): 0.46636577875190577\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "35706/35706 [==============================] - 12s 323us/step - loss: 0.3679 - acc: 0.8688 - val_loss: 0.3216 - val_acc: 0.8900\n",
      " - fnc_score: 0.8134603174603174\n",
      " - val_f1_(macro): 0.4972586397268793\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "35706/35706 [==============================] - 12s 324us/step - loss: 0.3624 - acc: 0.8702 - val_loss: 0.3172 - val_acc: 0.8928\n",
      " - fnc_score: 0.8166349206349206\n",
      " - val_f1_(macro): 0.4876772501855403\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "35706/35706 [==============================] - 12s 324us/step - loss: 0.3588 - acc: 0.8725 - val_loss: 0.3214 - val_acc: 0.8893\n",
      " - fnc_score: 0.7946666666666666\n",
      " - val_f1_(macro): 0.4654204617802328\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "35706/35706 [==============================] - 12s 323us/step - loss: 0.3499 - acc: 0.8746 - val_loss: 0.3166 - val_acc: 0.8928\n",
      " - fnc_score: 0.8186666666666667\n",
      " - val_f1_(macro): 0.4836976086554106\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "35706/35706 [==============================] - 12s 325us/step - loss: 0.3502 - acc: 0.8747 - val_loss: 0.3136 - val_acc: 0.8936\n",
      " - fnc_score: 0.8187936507936507\n",
      " - val_f1_(macro): 0.49570088335740314\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "35706/35706 [==============================] - 12s 325us/step - loss: 0.3473 - acc: 0.8752 - val_loss: 0.3095 - val_acc: 0.8945\n",
      " - fnc_score: 0.812063492063492\n",
      " - val_f1_(macro): 0.4802868317682799\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "35706/35706 [==============================] - 12s 322us/step - loss: 0.3429 - acc: 0.8766 - val_loss: 0.3210 - val_acc: 0.8932\n",
      " - fnc_score: 0.8194285714285714\n",
      " - val_f1_(macro): 0.48159751595987643\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "35706/35706 [==============================] - 12s 324us/step - loss: 0.3378 - acc: 0.8775 - val_loss: 0.3085 - val_acc: 0.8960\n",
      " - fnc_score: 0.8153650793650794\n",
      " - val_f1_(macro): 0.501216291497226\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "35706/35706 [==============================] - 12s 327us/step - loss: 0.3352 - acc: 0.8781 - val_loss: 0.3113 - val_acc: 0.8951\n",
      " - fnc_score: 0.8172698412698413\n",
      " - val_f1_(macro): 0.4926451134361132\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "35706/35706 [==============================] - 12s 324us/step - loss: 0.3317 - acc: 0.8789 - val_loss: 0.3074 - val_acc: 0.8953\n",
      " - fnc_score: 0.8228571428571428\n",
      " - val_f1_(macro): 0.49060954215581576\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (4644, 16)\n",
      "X_train_doc.shape = (4644, 28)\n",
      "Score for fold 8 was - 0.8228571428571428\n",
      "X_train_head.shape = (36502, 16)\n",
      "X_train_doc.shape = (36502, 28)\n",
      "X_train_head.shape = (3848, 16)\n",
      "X_train_doc.shape = (3848, 28)\n",
      "{0: 3.4936830015313936, 1: 15.44077834179357, 2: 1.4271973725367533, 3: 0.3391748745586322}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(36502, 44)\n",
      "ytrain shape:(36502,)\n",
      "Train on 36502 samples, validate on 3848 samples\n",
      "Epoch 1/100\n",
      "36502/36502 [==============================] - 17s 458us/step - loss: 0.5205 - acc: 0.8272 - val_loss: 0.4542 - val_acc: 0.8389\n",
      " - fnc_score: 0.7034607870499582\n",
      " - val_f1_(macro): 0.4611099849267556\n",
      "\n",
      "\n",
      "Epoch 2/100\n",
      "  500/36502 [..............................] - ETA: 10s - loss: 0.5230 - acc: 0.8240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36502/36502 [==============================] - 12s 324us/step - loss: 0.4185 - acc: 0.8549 - val_loss: 0.4334 - val_acc: 0.8119\n",
      " - fnc_score: 0.6890873569634385\n",
      " - val_f1_(macro): 0.4602936044279077\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "36502/36502 [==============================] - 12s 323us/step - loss: 0.4043 - acc: 0.8522 - val_loss: 0.3984 - val_acc: 0.8560\n",
      " - fnc_score: 0.7673737091822496\n",
      " - val_f1_(macro): 0.4487518054332438\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "36502/36502 [==============================] - 12s 323us/step - loss: 0.3802 - acc: 0.8650 - val_loss: 0.4488 - val_acc: 0.8404\n",
      " - fnc_score: 0.7968183086798772\n",
      " - val_f1_(macro): 0.4790469053449086\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "36502/36502 [==============================] - 12s 323us/step - loss: 0.3837 - acc: 0.8654 - val_loss: 0.3717 - val_acc: 0.8669\n",
      " - fnc_score: 0.7838403572425342\n",
      " - val_f1_(macro): 0.49621420569451624\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "36502/36502 [==============================] - 12s 324us/step - loss: 0.3632 - acc: 0.8714 - val_loss: 0.3762 - val_acc: 0.8669\n",
      " - fnc_score: 0.7677923527770025\n",
      " - val_f1_(macro): 0.48643029012544675\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "36502/36502 [==============================] - 12s 326us/step - loss: 0.3637 - acc: 0.8718 - val_loss: 0.3658 - val_acc: 0.8714\n",
      " - fnc_score: 0.7841194529723695\n",
      " - val_f1_(macro): 0.48822723064872176\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "36502/36502 [==============================] - 12s 322us/step - loss: 0.3526 - acc: 0.8737 - val_loss: 0.4557 - val_acc: 0.8365\n",
      " - fnc_score: 0.7966787608149596\n",
      " - val_f1_(macro): 0.47491078712100243\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "36502/36502 [==============================] - 12s 321us/step - loss: 0.3768 - acc: 0.8680 - val_loss: 0.3755 - val_acc: 0.8688\n",
      " - fnc_score: 0.7779793469159921\n",
      " - val_f1_(macro): 0.48674931248943426\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "36502/36502 [==============================] - 12s 322us/step - loss: 0.3537 - acc: 0.8753 - val_loss: 0.3631 - val_acc: 0.8708\n",
      " - fnc_score: 0.7823053307284399\n",
      " - val_f1_(macro): 0.49822396887625875\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "36502/36502 [==============================] - 12s 323us/step - loss: 0.3489 - acc: 0.8710 - val_loss: 0.3635 - val_acc: 0.8708\n",
      " - fnc_score: 0.7857940273513815\n",
      " - val_f1_(macro): 0.48326316425989835\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "36502/36502 [==============================] - 12s 323us/step - loss: 0.3432 - acc: 0.8780 - val_loss: 0.3689 - val_acc: 0.8701\n",
      " - fnc_score: 0.7968183086798772\n",
      " - val_f1_(macro): 0.5060253721876217\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "36502/36502 [==============================] - 12s 324us/step - loss: 0.3397 - acc: 0.8785 - val_loss: 0.3656 - val_acc: 0.8714\n",
      " - fnc_score: 0.7774211554563215\n",
      " - val_f1_(macro): 0.4777791245059302\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "36502/36502 [==============================] - 12s 321us/step - loss: 0.3339 - acc: 0.8791 - val_loss: 0.3634 - val_acc: 0.8719\n",
      " - fnc_score: 0.7997488138431482\n",
      " - val_f1_(macro): 0.4993368554544819\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "36502/36502 [==============================] - 12s 323us/step - loss: 0.3328 - acc: 0.8810 - val_loss: 0.3575 - val_acc: 0.8727\n",
      " - fnc_score: 0.7933296120569355\n",
      " - val_f1_(macro): 0.49892704648617914\n",
      "\n",
      "\n",
      "Epoch 16/100\n",
      "36502/36502 [==============================] - 12s 320us/step - loss: 0.3264 - acc: 0.8819 - val_loss: 0.3569 - val_acc: 0.8737\n",
      " - fnc_score: 0.7876081495953112\n",
      " - val_f1_(macro): 0.5027448037614541\n",
      "\n",
      "\n",
      "Epoch 17/100\n",
      "36502/36502 [==============================] - 12s 323us/step - loss: 0.3243 - acc: 0.8815 - val_loss: 0.3598 - val_acc: 0.8724\n",
      " - fnc_score: 0.7793748255651689\n",
      " - val_f1_(macro): 0.5042569325933615\n",
      "\n",
      "\n",
      "Epoch 18/100\n",
      "36502/36502 [==============================] - 12s 326us/step - loss: 0.3228 - acc: 0.8829 - val_loss: 0.3506 - val_acc: 0.8771\n",
      " - fnc_score: 0.7908177504884175\n",
      " - val_f1_(macro): 0.502192739935055\n",
      "\n",
      "\n",
      "Epoch 19/100\n",
      "36502/36502 [==============================] - 12s 322us/step - loss: 0.3180 - acc: 0.8841 - val_loss: 0.4668 - val_acc: 0.8329\n",
      " - fnc_score: 0.6735975439575774\n",
      " - val_f1_(macro): 0.4391656359861868\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (3848, 16)\n",
      "X_train_doc.shape = (3848, 28)\n",
      "Score for fold 9 was - 0.6735975439575774\n",
      "X_train_head.shape = (36077, 16)\n",
      "X_train_doc.shape = (36077, 28)\n",
      "X_train_head.shape = (4273, 16)\n",
      "X_train_doc.shape = (4273, 28)\n",
      "{0: 3.3996419148134187, 1: 14.957296849087895, 2: 1.4090376503671302, 3: 0.3413796366389099}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(36077, 44)\n",
      "ytrain shape:(36077,)\n",
      "Train on 36077 samples, validate on 4273 samples\n",
      "Epoch 1/100\n",
      "36077/36077 [==============================] - 20s 547us/step - loss: 0.5317 - acc: 0.8222 - val_loss: 0.3918 - val_acc: 0.8629\n",
      " - fnc_score: 0.7774929159357712\n",
      " - val_f1_(macro): 0.4141277607734708\n",
      "\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36077/36077 [==============================] - 15s 406us/step - loss: 0.4224 - acc: 0.8523 - val_loss: 0.3588 - val_acc: 0.8734\n",
      " - fnc_score: 0.7812710835244906\n",
      " - val_f1_(macro): 0.4634433163405685\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "36077/36077 [==============================] - 15s 402us/step - loss: 0.3969 - acc: 0.8597 - val_loss: 0.3727 - val_acc: 0.8689\n",
      " - fnc_score: 0.8121710970179463\n",
      " - val_f1_(macro): 0.46086470314231626\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "36077/36077 [==============================] - 14s 402us/step - loss: 0.3827 - acc: 0.8648 - val_loss: 0.3565 - val_acc: 0.8755\n",
      " - fnc_score: 0.8125759006881662\n",
      " - val_f1_(macro): 0.45803093742206125\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "36077/36077 [==============================] - 15s 405us/step - loss: 0.3737 - acc: 0.8678 - val_loss: 0.3379 - val_acc: 0.8870\n",
      " - fnc_score: 0.8137903116988261\n",
      " - val_f1_(macro): 0.47304642135039887\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "36077/36077 [==============================] - 14s 401us/step - loss: 0.3647 - acc: 0.8697 - val_loss: 0.3320 - val_acc: 0.8872\n",
      " - fnc_score: 0.8066387801916071\n",
      " - val_f1_(macro): 0.47396691350549236\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "36077/36077 [==============================] - 15s 406us/step - loss: 0.3603 - acc: 0.8720 - val_loss: 0.3522 - val_acc: 0.8795\n",
      " - fnc_score: 0.8199973013088652\n",
      " - val_f1_(macro): 0.4635018997908611\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "36077/36077 [==============================] - 14s 401us/step - loss: 0.3565 - acc: 0.8733 - val_loss: 0.3326 - val_acc: 0.8844\n",
      " - fnc_score: 0.8090676022129267\n",
      " - val_f1_(macro): 0.47779959383965565\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "36077/36077 [==============================] - 15s 402us/step - loss: 0.3533 - acc: 0.8734 - val_loss: 0.3278 - val_acc: 0.8886\n",
      " - fnc_score: 0.8055593037376872\n",
      " - val_f1_(macro): 0.4657339957891429\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "36077/36077 [==============================] - 15s 404us/step - loss: 0.3487 - acc: 0.8749 - val_loss: 0.3297 - val_acc: 0.8874\n",
      " - fnc_score: 0.8150047227094859\n",
      " - val_f1_(macro): 0.48104732731521926\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "36077/36077 [==============================] - 15s 403us/step - loss: 0.3444 - acc: 0.8762 - val_loss: 0.3277 - val_acc: 0.8884\n",
      " - fnc_score: 0.814464984482526\n",
      " - val_f1_(macro): 0.474693691171373\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "36077/36077 [==============================] - 15s 405us/step - loss: 0.3427 - acc: 0.8775 - val_loss: 0.3234 - val_acc: 0.8888\n",
      " - fnc_score: 0.8039400890568075\n",
      " - val_f1_(macro): 0.4783928949125967\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "36077/36077 [==============================] - 15s 403us/step - loss: 0.3374 - acc: 0.8778 - val_loss: 0.3274 - val_acc: 0.8909\n",
      " - fnc_score: 0.8120361624612064\n",
      " - val_f1_(macro): 0.4812625520856101\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "36077/36077 [==============================] - 15s 405us/step - loss: 0.3350 - acc: 0.8780 - val_loss: 0.3225 - val_acc: 0.8907\n",
      " - fnc_score: 0.8164890028336257\n",
      " - val_f1_(macro): 0.4707889336365038\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "36077/36077 [==============================] - 15s 406us/step - loss: 0.3296 - acc: 0.8798 - val_loss: 0.3213 - val_acc: 0.8916\n",
      " - fnc_score: 0.8178383484010255\n",
      " - val_f1_(macro): 0.48006301756153\n",
      "\n",
      "\n",
      "Epoch 16/100\n",
      "36077/36077 [==============================] - 15s 403us/step - loss: 0.3271 - acc: 0.8811 - val_loss: 0.3228 - val_acc: 0.8874\n",
      " - fnc_score: 0.8155444609364458\n",
      " - val_f1_(macro): 0.4656245025657895\n",
      "\n",
      "\n",
      "Epoch 17/100\n",
      "36077/36077 [==============================] - 15s 406us/step - loss: 0.3288 - acc: 0.8803 - val_loss: 0.3171 - val_acc: 0.8914\n",
      " - fnc_score: 0.8127108352449062\n",
      " - val_f1_(macro): 0.48398382946898255\n",
      "\n",
      "\n",
      "Epoch 18/100\n",
      "36077/36077 [==============================] - 15s 404us/step - loss: 0.3207 - acc: 0.8841 - val_loss: 0.3210 - val_acc: 0.8912\n",
      " - fnc_score: 0.8163540682768857\n",
      " - val_f1_(macro): 0.5026999495864857\n",
      "\n",
      "\n",
      "Epoch 19/100\n",
      "36077/36077 [==============================] - 14s 401us/step - loss: 0.3172 - acc: 0.8840 - val_loss: 0.3222 - val_acc: 0.8886\n",
      " - fnc_score: 0.8047496963972474\n",
      " - val_f1_(macro): 0.4816814417716485\n",
      "\n",
      "\n",
      "Epoch 20/100\n",
      "36077/36077 [==============================] - 14s 401us/step - loss: 0.3149 - acc: 0.8839 - val_loss: 0.3128 - val_acc: 0.8902\n",
      " - fnc_score: 0.8106868168938065\n",
      " - val_f1_(macro): 0.48447050024126403\n",
      "\n",
      "\n",
      "Epoch 21/100\n",
      "36077/36077 [==============================] - 15s 402us/step - loss: 0.3118 - acc: 0.8858 - val_loss: 0.3183 - val_acc: 0.8907\n",
      " - fnc_score: 0.8093374713264067\n",
      " - val_f1_(macro): 0.4869798753516953\n",
      "\n",
      "\n",
      "Epoch 22/100\n",
      "36077/36077 [==============================] - 15s 405us/step - loss: 0.3079 - acc: 0.8865 - val_loss: 0.3269 - val_acc: 0.8879\n",
      " - fnc_score: 0.8128457698016462\n",
      " - val_f1_(macro): 0.5139300919315043\n",
      "\n",
      "\n",
      "Epoch 23/100\n",
      "36077/36077 [==============================] - 15s 406us/step - loss: 0.3044 - acc: 0.8878 - val_loss: 0.3243 - val_acc: 0.8900\n",
      " - fnc_score: 0.8172986101740656\n",
      " - val_f1_(macro): 0.5100069255638913\n",
      "\n",
      "\n",
      "Epoch 24/100\n",
      "36077/36077 [==============================] - 15s 404us/step - loss: 0.3015 - acc: 0.8878 - val_loss: 0.3200 - val_acc: 0.8907\n",
      " - fnc_score: 0.8105518823370665\n",
      " - val_f1_(macro): 0.49353383966678555\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (4273, 16)\n",
      "X_train_doc.shape = (4273, 28)\n",
      "Score for fold 3 was - 0.8105518823370665\n",
      "X_train_head.shape = (36311, 16)\n",
      "X_train_doc.shape = (36311, 28)\n",
      "X_train_head.shape = (4039, 16)\n",
      "X_train_doc.shape = (4039, 28)\n",
      "{0: 3.456873571972582, 1: 14.784609120521173, 2: 1.4133193211894752, 3: 0.3406540828580006}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(36311, 44)\n",
      "ytrain shape:(36311,)\n",
      "Train on 36311 samples, validate on 4039 samples\n",
      "Epoch 1/100\n",
      "36311/36311 [==============================] - 20s 560us/step - loss: 0.5271 - acc: 0.8226 - val_loss: 0.4062 - val_acc: 0.8569\n",
      " - fnc_score: 0.7390697024724123\n",
      " - val_f1_(macro): 0.41542911444992614\n",
      "\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36311/36311 [==============================] - 15s 406us/step - loss: 0.4208 - acc: 0.8538 - val_loss: 0.3751 - val_acc: 0.8678\n",
      " - fnc_score: 0.7644922475206034\n",
      " - val_f1_(macro): 0.45561901533792004\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "36311/36311 [==============================] - 15s 401us/step - loss: 0.3941 - acc: 0.8614 - val_loss: 0.3705 - val_acc: 0.8730\n",
      " - fnc_score: 0.7700796200586674\n",
      " - val_f1_(macro): 0.4829602157639531\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "36311/36311 [==============================] - 15s 405us/step - loss: 0.3849 - acc: 0.8648 - val_loss: 0.3743 - val_acc: 0.8690\n",
      " - fnc_score: 0.7982958513758905\n",
      " - val_f1_(macro): 0.48060443321720836\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "36311/36311 [==============================] - 15s 406us/step - loss: 0.3731 - acc: 0.8677 - val_loss: 0.3433 - val_acc: 0.8821\n",
      " - fnc_score: 0.8016482748987289\n",
      " - val_f1_(macro): 0.49151277652986075\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "36311/36311 [==============================] - 15s 401us/step - loss: 0.3632 - acc: 0.8713 - val_loss: 0.3478 - val_acc: 0.8767\n",
      " - fnc_score: 0.7795781533733762\n",
      " - val_f1_(macro): 0.4846793180577207\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "36311/36311 [==============================] - 15s 405us/step - loss: 0.3589 - acc: 0.8728 - val_loss: 0.3396 - val_acc: 0.8782\n",
      " - fnc_score: 0.7846067886576338\n",
      " - val_f1_(macro): 0.481385648242619\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "36311/36311 [==============================] - 15s 405us/step - loss: 0.3539 - acc: 0.8746 - val_loss: 0.3435 - val_acc: 0.8799\n",
      " - fnc_score: 0.799832378823858\n",
      " - val_f1_(macro): 0.4867671945378438\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "36311/36311 [==============================] - 15s 403us/step - loss: 0.3473 - acc: 0.8769 - val_loss: 0.3384 - val_acc: 0.8802\n",
      " - fnc_score: 0.7980164827489873\n",
      " - val_f1_(macro): 0.4963084114810493\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "36311/36311 [==============================] - 15s 404us/step - loss: 0.3443 - acc: 0.8762 - val_loss: 0.3415 - val_acc: 0.8765\n",
      " - fnc_score: 0.7952227964799553\n",
      " - val_f1_(macro): 0.4920732251582933\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "36311/36311 [==============================] - 15s 403us/step - loss: 0.3413 - acc: 0.8778 - val_loss: 0.3347 - val_acc: 0.8799\n",
      " - fnc_score: 0.8041625925408576\n",
      " - val_f1_(macro): 0.4818901357539602\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (4039, 16)\n",
      "X_train_doc.shape = (4039, 28)\n",
      "Score for fold 1 was - 0.8041625925408576\n",
      "X_train_head.shape = (36406, 16)\n",
      "X_train_doc.shape = (36406, 28)\n",
      "X_train_head.shape = (3944, 16)\n",
      "X_train_doc.shape = (3944, 28)\n",
      "{0: 3.504620716211013, 1: 14.28806907378336, 2: 1.410210722032848, 3: 0.3406504977917509}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:255: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training \n",
      "xtrain shape:(36406, 44)\n",
      "ytrain shape:(36406,)\n",
      "Train on 36406 samples, validate on 3944 samples\n",
      "Epoch 1/100\n",
      "36406/36406 [==============================] - 21s 564us/step - loss: 0.5319 - acc: 0.8286 - val_loss: 0.3997 - val_acc: 0.8527\n",
      " - fnc_score: 0.7330090141651167\n",
      " - val_f1_(macro): 0.4053331799438811\n",
      "\n",
      "\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36406/36406 [==============================] - 15s 400us/step - loss: 0.4232 - acc: 0.8523 - val_loss: 0.3732 - val_acc: 0.8654\n",
      " - fnc_score: 0.7574760337673487\n",
      " - val_f1_(macro): 0.4570924855758262\n",
      "\n",
      "\n",
      "Epoch 3/100\n",
      "36406/36406 [==============================] - 15s 401us/step - loss: 0.3999 - acc: 0.8602 - val_loss: 0.3614 - val_acc: 0.8707\n",
      " - fnc_score: 0.7690656746315638\n",
      " - val_f1_(macro): 0.45595546495681105\n",
      "\n",
      "\n",
      "Epoch 4/100\n",
      "36406/36406 [==============================] - 15s 401us/step - loss: 0.3802 - acc: 0.8669 - val_loss: 0.3535 - val_acc: 0.8674\n",
      " - fnc_score: 0.7689225926455859\n",
      " - val_f1_(macro): 0.4595311532402958\n",
      "\n",
      "\n",
      "Epoch 5/100\n",
      "36406/36406 [==============================] - 15s 399us/step - loss: 0.3693 - acc: 0.8702 - val_loss: 0.3430 - val_acc: 0.8755\n",
      " - fnc_score: 0.7880955787666333\n",
      " - val_f1_(macro): 0.46324761449848917\n",
      "\n",
      "\n",
      "Epoch 6/100\n",
      "36406/36406 [==============================] - 15s 405us/step - loss: 0.3677 - acc: 0.8705 - val_loss: 0.3475 - val_acc: 0.8758\n",
      " - fnc_score: 0.7855201030190299\n",
      " - val_f1_(macro): 0.4511214908543933\n",
      "\n",
      "\n",
      "Epoch 7/100\n",
      "36406/36406 [==============================] - 15s 403us/step - loss: 0.3607 - acc: 0.8724 - val_loss: 0.3446 - val_acc: 0.8732\n",
      " - fnc_score: 0.7905279725282587\n",
      " - val_f1_(macro): 0.46502706370477054\n",
      "\n",
      "\n",
      "Epoch 8/100\n",
      "36406/36406 [==============================] - 15s 403us/step - loss: 0.3538 - acc: 0.8750 - val_loss: 0.3312 - val_acc: 0.8775\n",
      " - fnc_score: 0.7888109886965231\n",
      " - val_f1_(macro): 0.47064149794900967\n",
      "\n",
      "\n",
      "Epoch 9/100\n",
      "36406/36406 [==============================] - 15s 401us/step - loss: 0.3495 - acc: 0.8753 - val_loss: 0.3324 - val_acc: 0.8760\n",
      " - fnc_score: 0.7923880383459723\n",
      " - val_f1_(macro): 0.4649346028149083\n",
      "\n",
      "\n",
      "Epoch 10/100\n",
      "36406/36406 [==============================] - 15s 404us/step - loss: 0.3445 - acc: 0.8771 - val_loss: 0.3360 - val_acc: 0.8742\n",
      " - fnc_score: 0.7952496780655316\n",
      " - val_f1_(macro): 0.4653042675646457\n",
      "\n",
      "\n",
      "Epoch 11/100\n",
      "36406/36406 [==============================] - 15s 402us/step - loss: 0.3415 - acc: 0.8780 - val_loss: 0.3315 - val_acc: 0.8732\n",
      " - fnc_score: 0.7929603662898841\n",
      " - val_f1_(macro): 0.4565759434891099\n",
      "\n",
      "\n",
      "Epoch 12/100\n",
      "36406/36406 [==============================] - 15s 404us/step - loss: 0.3402 - acc: 0.8783 - val_loss: 0.3237 - val_acc: 0.8783\n",
      " - fnc_score: 0.7846616111031621\n",
      " - val_f1_(macro): 0.4592659192020327\n",
      "\n",
      "\n",
      "Epoch 13/100\n",
      "36406/36406 [==============================] - 15s 400us/step - loss: 0.3353 - acc: 0.8794 - val_loss: 0.3244 - val_acc: 0.8778\n",
      " - fnc_score: 0.789669480612391\n",
      " - val_f1_(macro): 0.46946917232154484\n",
      "\n",
      "\n",
      "Epoch 14/100\n",
      "36406/36406 [==============================] - 15s 403us/step - loss: 0.3315 - acc: 0.8804 - val_loss: 0.3313 - val_acc: 0.8745\n",
      " - fnc_score: 0.7905279725282587\n",
      " - val_f1_(macro): 0.4714178438395132\n",
      "\n",
      "\n",
      "Epoch 15/100\n",
      "36406/36406 [==============================] - 15s 402us/step - loss: 0.3281 - acc: 0.8813 - val_loss: 0.3257 - val_acc: 0.8747\n",
      " - fnc_score: 0.7941050221777078\n",
      " - val_f1_(macro): 0.507607094401737\n",
      "\n",
      "\n",
      "Epoch 16/100\n",
      "36406/36406 [==============================] - 15s 402us/step - loss: 0.3266 - acc: 0.8817 - val_loss: 0.3233 - val_acc: 0.8768\n",
      " - fnc_score: 0.7913864644441265\n",
      " - val_f1_(macro): 0.47099042213054443\n",
      "\n",
      "\n",
      "Epoch 17/100\n",
      "36406/36406 [==============================] - 15s 403us/step - loss: 0.3236 - acc: 0.8832 - val_loss: 0.3288 - val_acc: 0.8755\n",
      " - fnc_score: 0.7766490198883961\n",
      " - val_f1_(macro): 0.4634934602813586\n",
      "\n",
      "\n",
      "Training finished \n",
      "\n",
      "X_train_head.shape = (3944, 16)\n",
      "X_train_doc.shape = (3944, 28)\n",
      "Score for fold 4 was - 0.7766490198883961\n",
      "Scores on the dev set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    97     |     1     |    544    |    120    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    15     |     0     |    123    |    24     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    64     |     0     |   1450    |    286    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     1     |     0     |    74     |   6823    |\n",
      "-------------------------------------------------------------\n",
      "Score: 3439.5 out of 4448.5\t(77.31819714510509%)\n",
      "\n",
      "\n",
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    136    |     1     |   1382    |    384    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    27     |     1     |    396    |    273    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    203    |     0     |   3383    |    878    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     8     |     1     |    265    |   18075   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8541.0 out of 11651.25\t(73.30543933054393%)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from drive.FNC_challenge.feature_engineering import refuting_features, polarity_features, hand_features, gen_or_load_feats\n",
    "from drive.FNC_challenge.feature_engineering import word_overlap_features\n",
    "from drive.FNC_challenge.utils.dataset import DataSet\n",
    "from drive.FNC_challenge.utils.generate_test_splits import kfold_split, get_stances_for_folds\n",
    "from drive.FNC_challenge.utils.score import report_score, LABELS, score_submission\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model, load_model\n",
    "from drive.FNC_challenge.utils.system import parse_params, check_version\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from drive.FNC_challenge.utils.score import LABELS, score_submission\n",
    "\n",
    "\n",
    "def append_to_loss_monitor_file(text, filepath):\n",
    "    with open(filepath, 'a+') as the_file:\n",
    "        the_file.write(text+\"\\n\")\n",
    "\n",
    "class EarlyStoppingOnF1(Callback):\n",
    "    \"\"\"\n",
    "    Prints some metrics after each epoch in order to observe overfitting\n",
    "                https://github.com/fchollet/keras/issues/5794\n",
    "                custom metrics: https://github.com/fchollet/keras/issues/2607\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, epochs,\n",
    "                 X_test_claims,\n",
    "                 X_test_orig_docs,\n",
    "                 y_test, loss_filename, epsilon=0.0, min_epoch=15, X_test_nt=None):\n",
    "        self.epochs = epochs\n",
    "        self.patience = 2\n",
    "        self.counter = 0\n",
    "        self.prev_score = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.loss_filename = loss_filename\n",
    "        self.min_epoch = min_epoch\n",
    "        self.X_test_nt = X_test_nt\n",
    "        # self.print_train_f1 = print_train_f1\n",
    "\n",
    "        # self.X_train_claims = X_train_claims\n",
    "        # self.X_train_orig_docs = X_train_orig_docs\n",
    "        # self.X_train_evid = X_train_evid\n",
    "        # self.y_train = y_train\n",
    "\n",
    "        self.X_test_claims = X_test_claims\n",
    "        self.X_test_orig_docs = X_test_orig_docs\n",
    "        self.y_test = y_test\n",
    "        Callback.__init__(self)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch + 1 < self.epochs:\n",
    "            from sklearn.metrics import f1_score\n",
    "\n",
    "            # get prediction and convert into list\n",
    "            if type(self.X_test_orig_docs).__module__ == np.__name__ and type(self.X_test_nt).__module__ == np.__name__:\n",
    "                predicted_one_hot = self.model.predict([\n",
    "                    self.X_test_claims,\n",
    "                    self.X_test_orig_docs,\n",
    "                    self.X_test_nt\n",
    "                ])\n",
    "            elif type(self.X_test_orig_docs).__module__ == np.__name__:\n",
    "                predicted_one_hot = self.model.predict([\n",
    "                    self.X_test_claims,\n",
    "                    self.X_test_orig_docs,\n",
    "                ])\n",
    "            else:\n",
    "                predicted_one_hot = self.model.predict(self.X_test_claims)\n",
    "            predict = np.argmax(predicted_one_hot, axis=-1)\n",
    "\n",
    "            \"\"\"\n",
    "            predicted_one_hot_train = self.model.predict([self.X_train_claims, self.X_train_orig_docs, self.X_train_evid])\n",
    "            predict_train = np.argmax(predicted_one_hot_train, axis=-1)\n",
    "\n",
    "\n",
    "            # f1 for train data\n",
    "            f1_macro_train = \"\"\n",
    "            if self.print_train_f1 == True:\n",
    "                f1_0_train = f1_score(self.y_train, predict_train, labels=[0], average=None)\n",
    "                f1_1_train = f1_score(self.y_train, predict_train, labels=[1], average=None)\n",
    "                f1_macro_train = (f1_0_train[0] + f1_1_train[0]) / 2\n",
    "                print(\" - train_f1_(macro): \" + str(f1_macro_train))\"\"\"\n",
    "\n",
    "            predicted = [LABELS[int(a)] for a in predict]\n",
    "            actual = [LABELS[int(a)] for a in self.y_test]\n",
    "            # calc FNC score\n",
    "            fold_score, _ = score_submission(actual, predicted)\n",
    "            max_fold_score, _ = score_submission(actual, actual)\n",
    "            fnc_score = fold_score / max_fold_score\n",
    "            print(\" - fnc_score: \" + str(fnc_score))\n",
    "\n",
    "            # f1 for test data\n",
    "            f1_0 = f1_score(self.y_test, predict, labels=[0], average=None)\n",
    "            f1_1 = f1_score(self.y_test, predict, labels=[1], average=None)\n",
    "            f1_2 = f1_score(self.y_test, predict, labels=[2], average=None)\n",
    "            f1_3 = f1_score(self.y_test, predict, labels=[3], average=None)\n",
    "            f1_macro = (f1_0[0] + f1_1[0] + f1_2[0] + f1_3[0]) / 4\n",
    "            print(\" - val_f1_(macro): \" + str(f1_macro))\n",
    "            print(\"\\n\")\n",
    "\n",
    "            header = \"\"\n",
    "            values = \"\"\n",
    "            for key, value in logs.items():\n",
    "                header = header + key + \";\"\n",
    "                values = values + str(value) + \";\"\n",
    "            if epoch == 0:\n",
    "                values = \"\\n\" + header + \"val_f1_macro;\" + \"fnc_score;\" + \"\\n\" + values + str(f1_macro) + str(\n",
    "                    fnc_score) + \";\"\n",
    "            else:\n",
    "                values += str(f1_macro) + \";\" + str(fnc_score) + \";\"\n",
    "            append_to_loss_monitor_file(values, self.loss_filename)\n",
    "\n",
    "            if epoch >= self.min_epoch - 1:  # 9\n",
    "                if f1_macro + self.epsilon <= self.prev_score:\n",
    "                    self.counter += 1\n",
    "                else:\n",
    "                    self.counter = 0\n",
    "                if self.counter >= 2:\n",
    "                    self.model.stop_training = True\n",
    "            # print(\"Counter at \" + str(self.counter))\n",
    "            self.prev_score = f1_macro\n",
    "            # print(\"\\n\")\n",
    "\n",
    "def calculate_class_weight(y_train, no_classes=2):\n",
    "    # https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "    from sklearn.utils import class_weight\n",
    "\n",
    "    class_weight_list = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "    class_weights = {}\n",
    "    for i in range(no_classes):\n",
    "        class_weights[i] = class_weight_list[i]\n",
    "    print(class_weights)\n",
    "    return class_weights\n",
    "\n",
    "def convert_data_to_one_hot(y_train):\n",
    "    # y_test_temp = np.zeros((y_test.size, y_test.max() + 1), dtype=np.int)\n",
    "    # y_test_temp[np.arange(y_test.size), y_test] = 1\n",
    "\n",
    "    # Other option:\n",
    "    #   y_train is a tensor then because of one_hot, but feed_dict only accepts numpy arrays => replace y_train with sess.run(y_train)\n",
    "    #   http://stackoverflow.com/questions/34410654/tensorflow-valueerror-setting-an-array-element-with-a-sequence\n",
    "    # return tf.one_hot(y_train, 4), tf.one_hot(y_test, 4)\n",
    "    y_train_temp = np.zeros((y_train.size, y_train.max() + 1), dtype=np.int)\n",
    "    y_train_temp[np.arange(y_train.size), y_train] = 1\n",
    "\n",
    "    return y_train_temp\n",
    "\n",
    "def split_X(X_train, MAX_SEQ_LENGTH_HEADS):\n",
    "    # split to get [heads, docs]\n",
    "    X_train_splits = np.hsplit(X_train, np.array([MAX_SEQ_LENGTH_HEADS]))\n",
    "    X_train_head = X_train_splits[0]\n",
    "    X_train_doc = X_train_splits[1]\n",
    "\n",
    "    print(\"X_train_head.shape = \" + str(np.array(X_train_head).shape))\n",
    "    print(\"X_train_doc.shape = \" + str(np.array(X_train_doc).shape))\n",
    "\n",
    "    return X_train_head,X_train_doc\n",
    "\n",
    "def generate_features(stances,dataset,name):\n",
    "    h, b, y = [],[],[]\n",
    "\n",
    "    for stance in stances:\n",
    "        y.append(LABELS.index(stance['Stance']))\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"drive/FNC_challenge/features/overlap.\"+name+\".npy\")\n",
    "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"drive/FNC_challenge/features/refuting.\"+name+\".npy\")\n",
    "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"drive/FNC_challenge/features/polarity.\"+name+\".npy\")\n",
    "    X_hand = gen_or_load_feats(hand_features, h, b, \"drive/FNC_challenge/features/hand.\"+name+\".npy\")\n",
    "\n",
    "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]\n",
    "    return X,y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch_size = 500\n",
    "    epochs = 100\n",
    "    word_index = 100\n",
    "    EMBEDDING_DIM = 50\n",
    "    MAX_LENGTH = 16\n",
    "    LSTM_implementation = 2\n",
    "    embeddings_index = {}\n",
    "    from gensim.models import KeyedVectors\n",
    "    #\n",
    "    glove_file = 'drive/FNC_challenge/glove.twitter.27B.50d.txt'\n",
    "    tmp_file = 'drive/FNC_challenge/word2vec.txt'\n",
    "\n",
    "    vecmodel = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    word2idx = {'_PAD': 0}\n",
    "\n",
    "    vocab_list = [(k, vecmodel.wv[k]) for k, v in vecmodel.wv.vocab.items()]\n",
    "    embeddings_matrix = np.zeros((len(vecmodel.wv.vocab.items()) + 1, vecmodel.vector_size))\n",
    "    for i in range(len(vocab_list)):\n",
    "        word = vocab_list[i][0]\n",
    "        word2idx[word] = i + 1\n",
    "        embeddings_matrix[i + 1] = vocab_list[i][1]\n",
    "    print(embeddings_matrix.shape)\n",
    "\n",
    "    # embedding_layer = Embedding(len(vecmodel.wv.vocab.items()) + 1,\n",
    "    #                             EMBEDDING_DIM,\n",
    "    #                             weights=[embeddings_matrix],\n",
    "    #                             trainable=False,\n",
    "    #                             input_length=4\n",
    "    #                             )\n",
    "    # parse_params()\n",
    "    # competition_dataset = DataSet(\"competition_test\")\n",
    "    # X_competition, y_competition = generate_features(competition_dataset.stances, competition_dataset, \"competition\")\n",
    "    d = DataSet(\"train\",\"drive/FNC_challenge/fnc-1\")\n",
    "    folds, hold_out = kfold_split(d, n_folds=10, base_dir=\"drive/FNC_challenge/splits\")\n",
    "    fold_stances, hold_out_stances = get_stances_for_folds(d, folds, hold_out)\n",
    "\n",
    "    # Load the competition dataset\n",
    "    competition_dataset = DataSet(\"competition_test\",\"drive/FNC_challenge/fnc-1\")\n",
    "    X_competition, y_competition = generate_features(competition_dataset.stances, competition_dataset, \"competition\")\n",
    "    X_competition_LSTM, X_competition_MLP = split_X(X_competition, MAX_LENGTH)\n",
    "    y_competition_one_hot = convert_data_to_one_hot(np.array(y_competition))\n",
    "    Xs = dict()\n",
    "    ys = dict()\n",
    "\n",
    "    # Load/Precompute all features now\n",
    "    X_holdout, y_holdout = generate_features(hold_out_stances, d, \"holdout\")\n",
    "    X_holdout_LSTM, X_holdout_MLP = split_X(X_holdout, MAX_LENGTH)\n",
    "    y_holdout_one_hot = convert_data_to_one_hot(np.array(y_holdout))\n",
    "    for fold in fold_stances:\n",
    "        Xs[fold], ys[fold] = generate_features(fold_stances[fold], d, str(fold))\n",
    "\n",
    "    best_score = 0\n",
    "    best_fold = None\n",
    "\n",
    "    for fold in fold_stances:\n",
    "        ids = list(range(len(folds)))\n",
    "        del ids[fold]\n",
    "\n",
    "        X_train = np.vstack(tuple([Xs[i] for i in ids]))\n",
    "        y_train = np.hstack(tuple([ys[i] for i in ids]))\n",
    "\n",
    "        X_test = Xs[fold]\n",
    "        y_test = ys[fold]\n",
    "        y_train_one_hot = convert_data_to_one_hot(y_train)\n",
    "        y_test_one_hot = convert_data_to_one_hot(np.array(y_test))\n",
    "        X_train_LSTM, X_train_MLP = split_X(X_train, MAX_LENGTH)\n",
    "        X_test_LSTM, X_test_MLP = split_X(X_test, MAX_LENGTH)\n",
    "        class_weights = calculate_class_weight(y_train, no_classes=4)\n",
    "        lstm_input = Input(shape=(MAX_LENGTH,), dtype='int32', name='lstm_input')\n",
    "        embedding = Embedding(input_dim=len(vecmodel.wv.vocab.items())+1,  # lookup table size\n",
    "                              output_dim=EMBEDDING_DIM,  # output dim for each number in a sequence\n",
    "                              weights=[embeddings_matrix],\n",
    "                              input_length=MAX_LENGTH,  # receive sequences of MAX_SEQ_LENGTH_CLAIMS integers\n",
    "                              mask_zero=True,\n",
    "                              trainable=True)(lstm_input)\n",
    "\n",
    "        data_LSTM = LSTM(\n",
    "            100, return_sequences=True, stateful=False, dropout=0.2,\n",
    "            batch_input_shape=(batch_size, MAX_LENGTH, EMBEDDING_DIM),\n",
    "            input_shape=(MAX_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n",
    "        )(embedding)\n",
    "        data_LSTM = LSTM(\n",
    "            100, return_sequences=False, stateful=False, dropout=0.2,\n",
    "            batch_input_shape=(batch_size, MAX_LENGTH, EMBEDDING_DIM),\n",
    "            input_shape=(MAX_LENGTH, EMBEDDING_DIM), implementation=LSTM_implementation\n",
    "        )(data_LSTM)\n",
    "\n",
    "        mlp_input = Input(shape=(len(X_train_MLP[0]),), dtype='float32', name='mlp_input')\n",
    "\n",
    "        merged = concatenate([data_LSTM, mlp_input])\n",
    "\n",
    "        dense_mid = Dense(600, kernel_regularizer=None, kernel_initializer='glorot_uniform',\n",
    "                          activity_regularizer= None, activation='relu')(merged)\n",
    "        dense_mid = Dense(600, kernel_regularizer=None, kernel_initializer='glorot_uniform',\n",
    "                          activity_regularizer=None, activation='relu')(dense_mid)\n",
    "        dense_mid = Dense(600, kernel_regularizer=None, kernel_initializer='glorot_uniform',\n",
    "                          activity_regularizer=None, activation='relu')(dense_mid)\n",
    "        dense_out = Dense(4, activation='softmax', name='dense_out')(dense_mid)\n",
    "\n",
    "        model = Model(inputs=[lstm_input, mlp_input], outputs=[dense_out])\n",
    "\n",
    "        model.compile(optimizer=optimizers.Adam(), loss='kullback_leibler_divergence',  # categorial_crossentropy\n",
    "                           metrics=['accuracy']\n",
    "                           )\n",
    "\n",
    "        print(\"Starting training \")\n",
    "        print(\"xtrain shape:\" + str(np.array(X_train).shape))\n",
    "        print(\"ytrain shape:\" + str(np.array(y_train).shape))\n",
    "        model.fit([X_train_LSTM, X_train_MLP],\n",
    "                       y_train_one_hot,\n",
    "                       validation_data=([X_test_LSTM, X_test_MLP], y_test_one_hot),\n",
    "                       batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "                       callbacks=[\n",
    "                           EarlyStoppingOnF1(epochs,\n",
    "                                             X_test_LSTM, X_test_MLP, y_test,\n",
    "                                             'model_loss', epsilon=0.0, min_epoch=10),\n",
    "                       ]\n",
    "        )\n",
    "        print(\"Training finished \\n\")\n",
    "        X_test_LSTM, X_test_MLP = split_X(X_test, MAX_LENGTH)\n",
    "\n",
    "        predicted_one_hot = model.predict(x=[X_test_LSTM, X_test_MLP])\n",
    "        predicted = np.argmax(predicted_one_hot, axis=-1)\n",
    "        y_test = np.argmax(y_test_one_hot.tolist(), axis=-1)\n",
    "\n",
    "        predicted = [LABELS[int(a)] for a in predicted]\n",
    "        actual = [LABELS[int(a)] for a in y_test]\n",
    "        fold_score, _ = score_submission(actual, predicted)\n",
    "        max_fold_score, _ = score_submission(actual, actual)\n",
    "        score = fold_score / max_fold_score\n",
    "\n",
    "        print(\"Score for fold \" + str(fold) + \" was - \" + str(score))\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_fold = model\n",
    "     \n",
    "    predicted_one_hot = model.predict(x=[X_holdout_LSTM, X_holdout_MLP])\n",
    "    predicted = np.argmax(predicted_one_hot, axis=-1)\n",
    "    y_holdout = np.argmax(y_holdout_one_hot.tolist(), axis=-1)       \n",
    "    predicted = [LABELS[int(a)] for a in predicted]\n",
    "    actual = [LABELS[int(a)] for a in y_holdout]\n",
    "    print(\"Scores on the dev set\")\n",
    "    report_score(actual, predicted)\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Run on competition dataset\n",
    "    predicted_one_hot_competition = model.predict(x=[X_competition_LSTM, X_competition_MLP])\n",
    "    predicted = np.argmax(predicted_one_hot_competition, axis=-1)\n",
    "    y_competition = np.argmax(y_competition_one_hot.tolist(), axis=-1)  \n",
    "    predicted = [LABELS[int(a)] for a in predicted]\n",
    "    actual = [LABELS[int(a)] for a in y_competition]\n",
    "\n",
    "    print(\"Scores on the test set\")\n",
    "    report_score(actual, predicted)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "KTXwi1n4YMgc",
    "colab_type": "code",
    "outputId": "399980b1-536a-4e29-ac2c-e776cd36ed29",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.563196174405E12,
     "user_tz": 240.0,
     "elapsed": 23336.0,
     "user": {
      "displayName": "胡沐晗",
      "photoUrl": "",
      "userId": "13480278940063465236"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_head.shape = (25413, 16)\n",
      "X_train_doc.shape = (25413, 28)\n",
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    21     |     0     |    189    |    488    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     5     |     1     |    41     |    199    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    51     |     2     |    376    |   1249    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    229    |     9     |   1534    |   5228    |\n",
      "-------------------------------------------------------------\n",
      "Score: 1777.0 out of 11651.25\t(15.251582448235167%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15.251582448235167"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_competition_LSTM, X_competition_MLP = split_X(X_competition, MAX_LENGTH)\n",
    "predicted_one_hot_competition = model.predict(x=[X_competition_LSTM, X_competition_MLP])\n",
    "predicted = np.argmax(predicted_one_hot, axis=-1)\n",
    "y_competition_one_hot = convert_data_to_one_hot(np.array(y_competition))\n",
    "y_competition = np.argmax(y_competition_one_hot, axis=-1)\n",
    "predicted = [LABELS[int(a)] for a in predicted]\n",
    "\n",
    "\n",
    "actual = [LABELS[int(a)] for a in y_competition]\n",
    "\n",
    "print(\"Scores on the test set\")\n",
    "report_score(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "vSiFYMw5YFUY",
    "colab_type": "code",
    "outputId": "4aa16224-a5d2-42f9-b4a2-b2959d701422",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.56319621964E12,
     "user_tz": 240.0,
     "elapsed": 867.0,
     "user": {
      "displayName": "胡沐晗",
      "photoUrl": "",
      "userId": "13480278940063465236"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "s20dMv8DYDzz",
    "colab_type": "code",
    "outputId": "5b111cf2-4faf-4978-e27c-f4f4e8f7f6cd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.563196234949E12,
     "user_tz": 240.0,
     "elapsed": 866.0,
     "user": {
      "displayName": "胡沐晗",
      "photoUrl": "",
      "userId": "13480278940063465236"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000.0
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'agree',\n",
       " 'disagree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'disagree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'disagree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'disagree',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'agree',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'discuss',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " 'unrelated',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "project.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
